
Loading dataset Adult with preserved categories

Loading dataset Adult
Dataset info : 
task : classification
sample count (train + test) : 32561 with 22792 for training
number of features : 13 
 of which continuous : 5
One hot encoded features : False
number of classes : 2
class proportions : 
[0.75919044 0.24080956]


Benchmarking catboost_classifier on dataset Adult

fitted in 0.777s
predicted in 0.021s
ROC AUC: 0.9243, ACC: 0.8250
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3477
Average precision score: 0.8177
              precision    recall  f1-score   support

           0       0.95      0.81      0.88      7417
           1       0.59      0.87      0.70      2352

    accuracy                           0.82      9769
   macro avg       0.77      0.84      0.79      9769
weighted avg       0.86      0.82      0.83      9769


Benchmarking lightgbm_classifier on dataset Adult

fitted in 0.164s
predicted in 0.015s
ROC AUC: 0.9282, ACC: 0.8390
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3294
Average precision score: 0.8274
              precision    recall  f1-score   support

           0       0.95      0.83      0.89      7417
           1       0.62      0.86      0.72      2352

    accuracy                           0.84      9769
   macro avg       0.78      0.85      0.80      9769
weighted avg       0.87      0.84      0.85      9769


Loading dataset Adult with one-hot categories

Loading dataset Adult
Dataset info : 
task : classification
sample count (train + test) : 32561 with 22792 for training
number of features : 13 
 of which continuous : 5
One hot encoded features : True
number of classes : 2
class proportions : 
[0.75919044 0.24080956]


Benchmarking logistic_regression_classifier on dataset Adult

fitted in 1.701s
predicted in 0.015s
ROC AUC: 0.9037, ACC: 0.8059
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3970
Average precision score: 0.7521
              precision    recall  f1-score   support

           0       0.94      0.79      0.86      7417
           1       0.56      0.85      0.68      2352

    accuracy                           0.81      9769
   macro avg       0.75      0.82      0.77      9769
weighted avg       0.85      0.81      0.82      9769


Benchmarking xgboost_classifier on dataset Adult

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 2.388s
predicted in 0.016s
ROC AUC: 0.9262, ACC: 0.8384
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3264
Average precision score: 0.8232
              precision    recall  f1-score   support

           0       0.94      0.84      0.89      7417
           1       0.62      0.84      0.72      2352

    accuracy                           0.84      9769
   macro avg       0.78      0.84      0.80      9769
weighted avg       0.87      0.84      0.85      9769


Benchmarking sklearn_random_forest_classifier on dataset Adult

fitted in 0.468s
predicted in 0.043s
ROC AUC: 0.9018, ACC: 0.8529
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3747
Average precision score: 0.7578
              precision    recall  f1-score   support

           0       0.88      0.93      0.91      7417
           1       0.74      0.60      0.66      2352

    accuracy                           0.85      9769
   macro avg       0.81      0.77      0.78      9769
weighted avg       0.85      0.85      0.85      9769


Benchmarking wildwood_classifier on dataset Adult

fitted in 1.620s
predicted in 2.466s
ROC AUC: 0.9156, ACC: 0.8613
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3126
Average precision score: 0.7956
              precision    recall  f1-score   support

           0       0.88      0.95      0.91      7417
           1       0.78      0.60      0.67      2352

    accuracy                           0.86      9769
   macro avg       0.83      0.77      0.79      9769
weighted avg       0.86      0.86      0.85      9769


Loading dataset Bank with preserved categories

Loading dataset Bank
Dataset info : 
task : classification
sample count (train + test) : 45211 with 31647 for training
number of features : 16 
 of which continuous : 5
One hot encoded features : False
number of classes : 2
class proportions : 
[0.8830152 0.1169848]


Benchmarking catboost_classifier on dataset Bank

fitted in 0.862s
predicted in 0.041s
ROC AUC: 0.9265, ACC: 0.8461
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3434
Average precision score: 0.5917
              precision    recall  f1-score   support

           0       0.98      0.84      0.91     11977
           1       0.42      0.86      0.57      1587

    accuracy                           0.85     13564
   macro avg       0.70      0.85      0.74     13564
weighted avg       0.91      0.85      0.87     13564


Benchmarking lightgbm_classifier on dataset Bank

fitted in 0.215s
predicted in 0.021s
ROC AUC: 0.9336, ACC: 0.8665
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2954
Average precision score: 0.6112
              precision    recall  f1-score   support

           0       0.98      0.87      0.92     11977
           1       0.46      0.87      0.60      1587

    accuracy                           0.87     13564
   macro avg       0.72      0.87      0.76     13564
weighted avg       0.92      0.87      0.88     13564


Loading dataset Bank with one-hot categories

Loading dataset Bank
Dataset info : 
task : classification
sample count (train + test) : 45211 with 31647 for training
number of features : 51 
 of which continuous : 5
One hot encoded features : True
number of classes : 2
class proportions : 
[0.8830152 0.1169848]


Benchmarking logistic_regression_classifier on dataset Bank

fitted in 0.962s
predicted in 0.004s
ROC AUC: 0.9098, ACC: 0.8456
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.4201
Average precision score: 0.5516
              precision    recall  f1-score   support

           0       0.97      0.85      0.91     11977
           1       0.42      0.83      0.56      1587

    accuracy                           0.85     13564
   macro avg       0.70      0.84      0.73     13564
weighted avg       0.91      0.85      0.87     13564


Benchmarking xgboost_classifier on dataset Bank

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 1.927s
predicted in 0.011s
ROC AUC: 0.9297, ACC: 0.8753
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2778
Average precision score: 0.6237
              precision    recall  f1-score   support

           0       0.97      0.88      0.93     11977
           1       0.48      0.82      0.61      1587

    accuracy                           0.88     13564
   macro avg       0.73      0.85      0.77     13564
weighted avg       0.92      0.88      0.89     13564


Benchmarking sklearn_random_forest_classifier on dataset Bank

fitted in 0.523s
predicted in 0.046s
ROC AUC: 0.9291, ACC: 0.9035
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2162
Average precision score: 0.6048
              precision    recall  f1-score   support

           0       0.92      0.98      0.95     11977
           1       0.68      0.33      0.45      1587

    accuracy                           0.90     13564
   macro avg       0.80      0.66      0.70     13564
weighted avg       0.89      0.90      0.89     13564


Benchmarking wildwood_classifier on dataset Bank

fitted in 1.959s
predicted in 3.349s
ROC AUC: 0.9313, ACC: 0.9045
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2081
Average precision score: 0.6177
              precision    recall  f1-score   support

           0       0.92      0.98      0.95     11977
           1       0.67      0.36      0.47      1587

    accuracy                           0.90     13564
   macro avg       0.80      0.67      0.71     13564
weighted avg       0.89      0.90      0.89     13564


Loading dataset Car with preserved categories

Loading dataset Car
Dataset info : 
task : classification
sample count (train + test) : 1728 with 1209 for training
number of features : 6 
 of which continuous : 0
One hot encoded features : False
number of classes : 4
class proportions : 
[0.22222222 0.03993056 0.70023148 0.03761574]


Benchmarking catboost_classifier on dataset Car

fitted in 0.424s
predicted in 0.002s
ROC AUC: 0.9895, ACC: 0.9403
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1981
Average precision score: 0.9349
              precision    recall  f1-score   support

           0       0.85      0.93      0.89       115
           1       0.77      0.81      0.79        21
           2       0.99      0.95      0.97       363
           3       0.83      0.95      0.88        20

    accuracy                           0.94       519
   macro avg       0.86      0.91      0.88       519
weighted avg       0.94      0.94      0.94       519


Benchmarking lightgbm_classifier on dataset Car

fitted in 0.197s
predicted in 0.005s
ROC AUC: 0.9991, ACC: 0.9788
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.0740
Average precision score: 0.9944
              precision    recall  f1-score   support

           0       0.93      0.98      0.96       115
           1       0.95      0.95      0.95        21
           2       1.00      0.98      0.99       363
           3       0.91      1.00      0.95        20

    accuracy                           0.98       519
   macro avg       0.95      0.98      0.96       519
weighted avg       0.98      0.98      0.98       519


Loading dataset Car with one-hot categories

Loading dataset Car
Dataset info : 
task : classification
sample count (train + test) : 1728 with 1209 for training
number of features : 21 
 of which continuous : 0
One hot encoded features : True
number of classes : 4
class proportions : 
[0.22222222 0.03993056 0.70023148 0.03761574]


Benchmarking logistic_regression_classifier on dataset Car

fitted in 0.306s
predicted in 0.003s
ROC AUC: 0.9809, ACC: 0.8844
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2835
Average precision score: 0.8823
              precision    recall  f1-score   support

           0       0.73      0.90      0.80       115
           1       0.67      0.95      0.78        21
           2       1.00      0.87      0.93       363
           3       0.67      1.00      0.80        20

    accuracy                           0.88       519
   macro avg       0.76      0.93      0.83       519
weighted avg       0.91      0.88      0.89       519


Benchmarking xgboost_classifier on dataset Car

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 0.274s
predicted in 0.002s
ROC AUC: 0.9995, ACC: 0.9884
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.0451
Average precision score: 0.9944
              precision    recall  f1-score   support

           0       0.97      0.98      0.98       115
           1       0.88      1.00      0.93        21
           2       1.00      0.99      1.00       363
           3       1.00      0.95      0.97        20

    accuracy                           0.99       519
   macro avg       0.96      0.98      0.97       519
weighted avg       0.99      0.99      0.99       519


Benchmarking sklearn_random_forest_classifier on dataset Car

fitted in 0.132s
predicted in 0.018s
ROC AUC: 0.9961, ACC: 0.9557
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2000
Average precision score: 0.9742
              precision    recall  f1-score   support

           0       0.87      0.96      0.91       115
           1       0.84      0.76      0.80        21
           2       0.99      0.97      0.98       363
           3       0.95      0.90      0.92        20

    accuracy                           0.96       519
   macro avg       0.91      0.90      0.90       519
weighted avg       0.96      0.96      0.96       519


Benchmarking wildwood_classifier on dataset Car

fitted in 0.073s
predicted in 0.041s
ROC AUC: 0.9895, ACC: 0.8940
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3756
Average precision score: 0.9288
              precision    recall  f1-score   support

           0       0.73      0.86      0.79       115
           1       1.00      0.05      0.09        21
           2       0.96      0.97      0.96       363
           3       0.85      0.55      0.67        20

    accuracy                           0.89       519
   macro avg       0.88      0.61      0.63       519
weighted avg       0.90      0.89      0.88       519


Loading dataset Cardio with preserved categories

Loading dataset Cardio
Dataset info : 
task : classification
sample count (train + test) : 2126 with 1488 for training
number of features : 24 
 of which continuous : 23
One hot encoded features : False
number of classes : 3
class proportions : 
[0.7784572  0.13875823 0.08278457]


Benchmarking catboost_classifier on dataset Cardio

fitted in 0.331s
predicted in 0.002s
ROC AUC: 0.9862, ACC: 0.9498
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1587
Average precision score: 0.9546
              precision    recall  f1-score   support

           0       0.97      0.97      0.97       497
           1       0.82      0.88      0.85        88
           2       0.96      0.89      0.92        53

    accuracy                           0.95       638
   macro avg       0.92      0.91      0.91       638
weighted avg       0.95      0.95      0.95       638


Benchmarking lightgbm_classifier on dataset Cardio

fitted in 0.319s
predicted in 0.009s
ROC AUC: 0.9881, ACC: 0.9624
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1706
Average precision score: 0.9612
              precision    recall  f1-score   support

           0       0.97      0.99      0.98       497
           1       0.93      0.84      0.88        88
           2       0.96      0.91      0.93        53

    accuracy                           0.96       638
   macro avg       0.95      0.91      0.93       638
weighted avg       0.96      0.96      0.96       638


Loading dataset Cardio with one-hot categories

Loading dataset Cardio
Dataset info : 
task : classification
sample count (train + test) : 2126 with 1488 for training
number of features : 24 
 of which continuous : 23
One hot encoded features : True
number of classes : 3
class proportions : 
[0.7784572  0.13875823 0.08278457]


Benchmarking logistic_regression_classifier on dataset Cardio

fitted in 0.331s
predicted in 0.002s
ROC AUC: 0.9486, ACC: 0.8574
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3719
Average precision score: 0.7921
              precision    recall  f1-score   support

           0       0.98      0.87      0.92       497
           1       0.52      0.77      0.62        88
           2       0.72      0.87      0.79        53

    accuracy                           0.86       638
   macro avg       0.74      0.84      0.78       638
weighted avg       0.89      0.86      0.87       638


Benchmarking xgboost_classifier on dataset Cardio

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 0.365s
predicted in 0.005s
ROC AUC: 0.9895, ACC: 0.9483
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1512
Average precision score: 0.9642
              precision    recall  f1-score   support

           0       0.97      0.98      0.97       497
           1       0.88      0.81      0.84        88
           2       0.89      0.89      0.89        53

    accuracy                           0.95       638
   macro avg       0.91      0.89      0.90       638
weighted avg       0.95      0.95      0.95       638


Benchmarking sklearn_random_forest_classifier on dataset Cardio

fitted in 0.135s
predicted in 0.018s
ROC AUC: 0.9854, ACC: 0.9436
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1661
Average precision score: 0.9489
              precision    recall  f1-score   support

           0       0.95      0.98      0.97       497
           1       0.86      0.76      0.81        88
           2       0.98      0.87      0.92        53

    accuracy                           0.94       638
   macro avg       0.93      0.87      0.90       638
weighted avg       0.94      0.94      0.94       638


Benchmarking wildwood_classifier on dataset Cardio

fitted in 0.077s
predicted in 0.083s
ROC AUC: 0.9827, ACC: 0.9326
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2114
Average precision score: 0.9459
              precision    recall  f1-score   support

           0       0.95      0.98      0.97       497
           1       0.81      0.74      0.77        88
           2       0.93      0.81      0.87        53

    accuracy                           0.93       638
   macro avg       0.90      0.84      0.87       638
weighted avg       0.93      0.93      0.93       638


Loading dataset Churn with preserved categories

Loading dataset Churn
Dataset info : 
task : classification
sample count (train + test) : 3333 with 2333 for training
number of features : 19 
 of which continuous : 15
One hot encoded features : False
number of classes : 2
class proportions : 
[0.85508551 0.14491449]


Benchmarking catboost_classifier on dataset Churn

fitted in 0.242s
predicted in 0.003s
ROC AUC: 0.9413, ACC: 0.9300
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2526
Average precision score: 0.8927
              precision    recall  f1-score   support

           0       0.98      0.94      0.96       855
           1       0.71      0.88      0.78       145

    accuracy                           0.93      1000
   macro avg       0.84      0.91      0.87      1000
weighted avg       0.94      0.93      0.93      1000


Benchmarking lightgbm_classifier on dataset Churn

fitted in 0.104s
predicted in 0.003s
ROC AUC: 0.9245, ACC: 0.9500
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1656
Average precision score: 0.8865
              precision    recall  f1-score   support

           0       0.97      0.97      0.97       855
           1       0.82      0.84      0.83       145

    accuracy                           0.95      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.95      0.95      0.95      1000


Loading dataset Churn with one-hot categories

Loading dataset Churn
Dataset info : 
task : classification
sample count (train + test) : 3333 with 2333 for training
number of features : 19 
 of which continuous : 15
One hot encoded features : True
number of classes : 2
class proportions : 
[0.85508551 0.14491449]


Benchmarking logistic_regression_classifier on dataset Churn

fitted in 0.459s
predicted in 0.002s
ROC AUC: 0.8055, ACC: 0.7530
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.5311
Average precision score: 0.4451
              precision    recall  f1-score   support

           0       0.94      0.76      0.84       855
           1       0.33      0.69      0.45       145

    accuracy                           0.75      1000
   macro avg       0.63      0.73      0.64      1000
weighted avg       0.85      0.75      0.78      1000


Benchmarking xgboost_classifier on dataset Churn

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 0.267s
predicted in 0.002s
ROC AUC: 0.9384, ACC: 0.9600
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1534
Average precision score: 0.9011
              precision    recall  f1-score   support

           0       0.97      0.98      0.98       855
           1       0.87      0.85      0.86       145

    accuracy                           0.96      1000
   macro avg       0.92      0.91      0.92      1000
weighted avg       0.96      0.96      0.96      1000


Benchmarking sklearn_random_forest_classifier on dataset Churn

fitted in 0.142s
predicted in 0.019s
ROC AUC: 0.9289, ACC: 0.9390
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2644
Average precision score: 0.8715
              precision    recall  f1-score   support

           0       0.94      0.99      0.97       855
           1       0.94      0.62      0.75       145

    accuracy                           0.94      1000
   macro avg       0.94      0.81      0.86      1000
weighted avg       0.94      0.94      0.93      1000


Benchmarking wildwood_classifier on dataset Churn

fitted in 0.192s
predicted in 0.185s
ROC AUC: 0.9338, ACC: 0.9350
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2178
Average precision score: 0.8735
              precision    recall  f1-score   support

           0       0.94      0.99      0.96       855
           1       0.93      0.59      0.73       145

    accuracy                           0.94      1000
   macro avg       0.93      0.79      0.84      1000
weighted avg       0.93      0.94      0.93      1000


Loading dataset Default_cb with preserved categories

Loading dataset Default_cb
Dataset info : 
task : classification
sample count (train + test) : 30000 with 21000 for training
number of features : 23 
 of which continuous : 14
One hot encoded features : False
number of classes : 2
class proportions : 
[0.7788 0.2212]


Benchmarking catboost_classifier on dataset Default_cb

fitted in 1.398s
predicted in 0.024s
ROC AUC: 0.7651, ACC: 0.7471
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.5434
Average precision score: 0.5298
              precision    recall  f1-score   support

           0       0.88      0.79      0.83      7009
           1       0.45      0.61      0.52      1991

    accuracy                           0.75      9000
   macro avg       0.66      0.70      0.67      9000
weighted avg       0.78      0.75      0.76      9000


Benchmarking lightgbm_classifier on dataset Default_cb

fitted in 0.222s
predicted in 0.013s
ROC AUC: 0.7737, ACC: 0.7528
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.5298
Average precision score: 0.5477
              precision    recall  f1-score   support

           0       0.88      0.79      0.83      7009
           1       0.46      0.61      0.52      1991

    accuracy                           0.75      9000
   macro avg       0.67      0.70      0.68      9000
weighted avg       0.78      0.75      0.76      9000


Loading dataset Default_cb with one-hot categories

Loading dataset Default_cb
Dataset info : 
task : classification
sample count (train + test) : 30000 with 21000 for training
number of features : 23 
 of which continuous : 14
One hot encoded features : True
number of classes : 2
class proportions : 
[0.7788 0.2212]


Benchmarking logistic_regression_classifier on dataset Default_cb

fitted in 0.323s
predicted in 0.003s
ROC AUC: 0.7128, ACC: 0.6889
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.6049
Average precision score: 0.4969
              precision    recall  f1-score   support

           0       0.87      0.71      0.78      7009
           1       0.38      0.63      0.47      1991

    accuracy                           0.69      9000
   macro avg       0.62      0.67      0.63      9000
weighted avg       0.76      0.69      0.71      9000


Benchmarking xgboost_classifier on dataset Default_cb

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 1.477s
predicted in 0.007s
ROC AUC: 0.7525, ACC: 0.7544
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.5290
Average precision score: 0.5197
              precision    recall  f1-score   support

           0       0.87      0.81      0.84      7009
           1       0.46      0.57      0.51      1991

    accuracy                           0.75      9000
   macro avg       0.66      0.69      0.67      9000
weighted avg       0.78      0.75      0.76      9000


Benchmarking sklearn_random_forest_classifier on dataset Default_cb

fitted in 0.747s
predicted in 0.037s
ROC AUC: 0.7586, ACC: 0.8114
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.4564
Average precision score: 0.5239
              precision    recall  f1-score   support

           0       0.83      0.95      0.89      7009
           1       0.64      0.33      0.44      1991

    accuracy                           0.81      9000
   macro avg       0.74      0.64      0.66      9000
weighted avg       0.79      0.81      0.79      9000


Benchmarking wildwood_classifier on dataset Default_cb

fitted in 1.442s
predicted in 2.324s
ROC AUC: 0.7709, ACC: 0.8188
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.4352
Average precision score: 0.5513
              precision    recall  f1-score   support

           0       0.84      0.95      0.89      7009
           1       0.67      0.35      0.46      1991

    accuracy                           0.82      9000
   macro avg       0.76      0.65      0.68      9000
weighted avg       0.80      0.82      0.80      9000


Loading dataset Letter with preserved categories

Loading dataset Letter
Dataset info : 
task : classification
sample count (train + test) : 20000 with 14000 for training
number of features : 16 
 of which continuous : 16
One hot encoded features : False
number of classes : 26
class proportions : 
[0.03945 0.0383  0.0368  0.04025 0.0384  0.03875 0.03865 0.0367  0.03775
 0.03735 0.03695 0.03805 0.0396  0.03915 0.03765 0.04015 0.03915 0.0379
 0.0374  0.0398  0.04065 0.0382  0.0376  0.03935 0.0393  0.0367 ]


Benchmarking catboost_classifier on dataset Letter

fitted in 2.238s
predicted in 0.021s
ROC AUC: 0.9991, ACC: 0.9382
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2020
Average precision score: 0.9848
              precision    recall  f1-score   support

           0       0.99      0.99      0.99       237
           1       0.90      0.91      0.91       230
           2       0.94      0.93      0.93       221
           3       0.89      0.95      0.92       241
           4       0.90      0.92      0.91       230
           5       0.92      0.93      0.93       233
           6       0.94      0.91      0.92       232
           7       0.94      0.89      0.92       220
           8       0.91      0.94      0.93       226
           9       0.95      0.91      0.93       224
          10       0.93      0.91      0.92       222
          11       0.98      0.96      0.97       228
          12       0.93      0.97      0.95       238
          13       0.91      0.94      0.93       235
          14       0.88      0.94      0.91       226
          15       0.96      0.91      0.94       241
          16       0.95      0.95      0.95       235
          17       0.91      0.87      0.89       227
          18       0.96      0.95      0.95       224
          19       0.95      0.95      0.95       239
          20       0.97      0.95      0.96       244
          21       0.94      0.94      0.94       229
          22       0.99      0.97      0.98       226
          23       0.95      0.95      0.95       236
          24       0.95      0.97      0.96       236
          25       0.97      0.97      0.97       220

    accuracy                           0.94      6000
   macro avg       0.94      0.94      0.94      6000
weighted avg       0.94      0.94      0.94      6000


Benchmarking lightgbm_classifier on dataset Letter

fitted in 2.441s
predicted in 0.252s
ROC AUC: 0.9995, ACC: 0.9617
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1337
Average precision score: 0.9923
              precision    recall  f1-score   support

           0       0.99      1.00      0.99       237
           1       0.92      0.96      0.94       230
           2       0.98      0.97      0.97       221
           3       0.93      0.97      0.95       241
           4       0.96      0.97      0.96       230
           5       0.96      0.95      0.96       233
           6       0.97      0.97      0.97       232
           7       0.95      0.91      0.93       220
           8       0.94      0.94      0.94       226
           9       0.95      0.92      0.94       224
          10       0.93      0.92      0.93       222
          11       1.00      0.96      0.98       228
          12       0.96      0.97      0.97       238
          13       0.94      0.97      0.95       235
          14       0.92      0.98      0.95       226
          15       0.97      0.96      0.97       241
          16       0.96      0.96      0.96       235
          17       0.94      0.90      0.92       227
          18       0.98      0.97      0.98       224
          19       0.97      0.98      0.98       239
          20       0.98      0.97      0.97       244
          21       0.97      0.95      0.96       229
          22       1.00      0.99      0.99       226
          23       0.97      0.99      0.98       236
          24       0.98      0.98      0.98       236
          25       0.98      0.99      0.98       220

    accuracy                           0.96      6000
   macro avg       0.96      0.96      0.96      6000
weighted avg       0.96      0.96      0.96      6000


Loading dataset Letter with one-hot categories

Loading dataset Letter
Dataset info : 
task : classification
sample count (train + test) : 20000 with 14000 for training
number of features : 16 
 of which continuous : 16
One hot encoded features : True
number of classes : 26
class proportions : 
[0.03945 0.0383  0.0368  0.04025 0.0384  0.03875 0.03865 0.0367  0.03775
 0.03735 0.03695 0.03805 0.0396  0.03915 0.03765 0.04015 0.03915 0.0379
 0.0374  0.0398  0.04065 0.0382  0.0376  0.03935 0.0393  0.0367 ]


Benchmarking logistic_regression_classifier on dataset Letter

fitted in 0.913s
predicted in 0.004s
ROC AUC: 0.9740, ACC: 0.7532
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 1.0885
Average precision score: 0.7826
              precision    recall  f1-score   support

           0       0.85      0.91      0.88       237
           1       0.69      0.83      0.75       230
           2       0.77      0.79      0.78       221
           3       0.71      0.85      0.78       241
           4       0.75      0.78      0.76       230
           5       0.79      0.70      0.74       233
           6       0.62      0.50      0.56       232
           7       0.52      0.45      0.48       220
           8       0.88      0.81      0.85       226
           9       0.89      0.78      0.83       224
          10       0.65      0.67      0.66       222
          11       0.89      0.76      0.82       228
          12       0.77      0.88      0.82       238
          13       0.82      0.81      0.82       235
          14       0.63      0.62      0.63       226
          15       0.88      0.78      0.83       241
          16       0.72      0.75      0.73       235
          17       0.72      0.74      0.73       227
          18       0.48      0.47      0.48       224
          19       0.79      0.82      0.81       239
          20       0.86      0.82      0.84       244
          21       0.82      0.87      0.84       229
          22       0.79      0.85      0.81       226
          23       0.74      0.80      0.77       236
          24       0.80      0.69      0.74       236
          25       0.74      0.81      0.77       220

    accuracy                           0.75      6000
   macro avg       0.75      0.75      0.75      6000
weighted avg       0.75      0.75      0.75      6000


Benchmarking xgboost_classifier on dataset Letter

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 10.925s
predicted in 0.074s
ROC AUC: 0.9994, ACC: 0.9557
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1417
Average precision score: 0.9908
              precision    recall  f1-score   support

           0       0.98      1.00      0.99       237
           1       0.92      0.93      0.92       230
           2       0.98      0.96      0.97       221
           3       0.93      0.95      0.94       241
           4       0.95      0.96      0.96       230
           5       0.94      0.95      0.95       233
           6       0.96      0.95      0.95       232
           7       0.94      0.93      0.94       220
           8       0.94      0.95      0.94       226
           9       0.96      0.92      0.94       224
          10       0.94      0.94      0.94       222
          11       1.00      0.95      0.97       228
          12       0.96      0.97      0.97       238
          13       0.92      0.95      0.94       235
          14       0.90      0.97      0.94       226
          15       0.97      0.96      0.96       241
          16       0.95      0.96      0.95       235
          17       0.95      0.92      0.93       227
          18       0.97      0.95      0.96       224
          19       0.97      0.96      0.96       239
          20       0.98      0.96      0.97       244
          21       0.96      0.96      0.96       229
          22       0.99      0.97      0.98       226
          23       0.96      0.97      0.97       236
          24       0.97      0.97      0.97       236
          25       0.98      0.98      0.98       220

    accuracy                           0.96      6000
   macro avg       0.96      0.96      0.96      6000
weighted avg       0.96      0.96      0.96      6000


Benchmarking sklearn_random_forest_classifier on dataset Letter

fitted in 0.308s
predicted in 0.034s
ROC AUC: 0.9994, ACC: 0.9593
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2879
Average precision score: 0.9893
              precision    recall  f1-score   support

           0       0.98      1.00      0.99       237
           1       0.90      0.95      0.92       230
           2       0.97      0.96      0.97       221
           3       0.93      0.97      0.95       241
           4       0.95      0.97      0.96       230
           5       0.96      0.95      0.96       233
           6       0.95      0.97      0.96       232
           7       0.96      0.90      0.92       220
           8       0.93      0.95      0.94       226
           9       0.95      0.92      0.94       224
          10       0.94      0.93      0.94       222
          11       1.00      0.98      0.99       228
          12       0.97      0.99      0.98       238
          13       0.94      0.97      0.95       235
          14       0.91      0.96      0.94       226
          15       0.98      0.96      0.97       241
          16       0.96      0.95      0.96       235
          17       0.93      0.90      0.91       227
          18       0.97      0.95      0.96       224
          19       0.97      0.98      0.98       239
          20       0.99      0.96      0.97       244
          21       0.96      0.95      0.95       229
          22       0.98      0.99      0.99       226
          23       0.97      0.97      0.97       236
          24       0.99      0.99      0.99       236
          25       0.99      0.98      0.98       220

    accuracy                           0.96      6000
   macro avg       0.96      0.96      0.96      6000
weighted avg       0.96      0.96      0.96      6000


Benchmarking wildwood_classifier on dataset Letter

fitted in 1.125s
predicted in 1.196s
ROC AUC: 0.9991, ACC: 0.9455
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.8380
Average precision score: 0.9834
              precision    recall  f1-score   support

           0       0.97      1.00      0.98       237
           1       0.90      0.93      0.92       230
           2       0.97      0.94      0.95       221
           3       0.92      0.98      0.95       241
           4       0.93      0.93      0.93       230
           5       0.94      0.94      0.94       233
           6       0.93      0.95      0.94       232
           7       0.95      0.86      0.90       220
           8       0.94      0.95      0.94       226
           9       0.97      0.91      0.94       224
          10       0.93      0.91      0.92       222
          11       1.00      0.96      0.98       228
          12       0.94      0.97      0.95       238
          13       0.95      0.94      0.94       235
          14       0.88      0.96      0.92       226
          15       0.97      0.93      0.95       241
          16       0.93      0.95      0.94       235
          17       0.87      0.92      0.89       227
          18       0.94      0.93      0.93       224
          19       0.97      0.97      0.97       239
          20       0.97      0.93      0.95       244
          21       0.97      0.95      0.96       229
          22       0.98      0.98      0.98       226
          23       0.94      0.96      0.95       236
          24       0.97      0.97      0.97       236
          25       0.96      0.98      0.97       220

    accuracy                           0.95      6000
   macro avg       0.95      0.95      0.95      6000
weighted avg       0.95      0.95      0.95      6000


Loading dataset Satimage with preserved categories

Loading dataset Satimage
Dataset info : 
task : classification
sample count (train + test) : 5104 with 3572 for training
number of features : 36 
 of which continuous : 36
One hot encoded features : False
number of classes : 6
class proportions : 
[0.24333856 0.11912226 0.23413009 0.1012931  0.10403605 0.19807994]


Benchmarking catboost_classifier on dataset Satimage

fitted in 0.491s
predicted in 0.003s
ROC AUC: 0.9884, ACC: 0.9073
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2486
Average precision score: 0.9439
              precision    recall  f1-score   support

           0       0.98      0.97      0.98       373
           1       0.97      0.99      0.98       183
           2       0.90      0.94      0.92       359
           3       0.69      0.70      0.69       155
           4       0.92      0.90      0.91       159
           5       0.89      0.86      0.87       303

    accuracy                           0.91      1532
   macro avg       0.89      0.89      0.89      1532
weighted avg       0.91      0.91      0.91      1532


Benchmarking lightgbm_classifier on dataset Satimage

fitted in 0.533s
predicted in 0.017s
ROC AUC: 0.9907, ACC: 0.9132
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3103
Average precision score: 0.9535
              precision    recall  f1-score   support

           0       0.98      0.98      0.98       373
           1       0.97      0.99      0.98       183
           2       0.90      0.94      0.92       359
           3       0.76      0.66      0.70       155
           4       0.94      0.87      0.90       159
           5       0.88      0.90      0.89       303

    accuracy                           0.91      1532
   macro avg       0.90      0.89      0.90      1532
weighted avg       0.91      0.91      0.91      1532


Loading dataset Satimage with one-hot categories

Loading dataset Satimage
Dataset info : 
task : classification
sample count (train + test) : 5104 with 3572 for training
number of features : 36 
 of which continuous : 36
One hot encoded features : True
number of classes : 6
class proportions : 
[0.24333856 0.11912226 0.23413009 0.1012931  0.10403605 0.19807994]


Benchmarking logistic_regression_classifier on dataset Satimage

fitted in 0.106s
predicted in 0.001s
ROC AUC: 0.9725, ACC: 0.8473
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.4256
Average precision score: 0.8567
              precision    recall  f1-score   support

           0       0.96      0.95      0.96       373
           1       0.95      0.95      0.95       183
           2       0.92      0.86      0.89       359
           3       0.51      0.70      0.59       155
           4       0.77      0.74      0.75       159
           5       0.85      0.77      0.81       303

    accuracy                           0.85      1532
   macro avg       0.82      0.83      0.82      1532
weighted avg       0.86      0.85      0.85      1532


Benchmarking xgboost_classifier on dataset Satimage

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:03:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 1.131s
predicted in 0.005s
ROC AUC: 0.9903, ACC: 0.9112
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2619
Average precision score: 0.9529
              precision    recall  f1-score   support

           0       0.98      0.98      0.98       373
           1       0.97      0.98      0.97       183
           2       0.91      0.94      0.92       359
           3       0.76      0.65      0.70       155
           4       0.92      0.89      0.90       159
           5       0.87      0.89      0.88       303

    accuracy                           0.91      1532
   macro avg       0.90      0.89      0.89      1532
weighted avg       0.91      0.91      0.91      1532


Benchmarking sklearn_random_forest_classifier on dataset Satimage

fitted in 0.183s
predicted in 0.018s
ROC AUC: 0.9900, ACC: 0.9106
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2604
Average precision score: 0.9484
              precision    recall  f1-score   support

           0       0.97      0.98      0.97       373
           1       0.96      0.98      0.97       183
           2       0.89      0.97      0.93       359
           3       0.82      0.61      0.70       155
           4       0.94      0.83      0.88       159
           5       0.86      0.90      0.88       303

    accuracy                           0.91      1532
   macro avg       0.91      0.88      0.89      1532
weighted avg       0.91      0.91      0.91      1532


Benchmarking wildwood_classifier on dataset Satimage

fitted in 0.275s
predicted in 0.225s
ROC AUC: 0.9874, ACC: 0.9054
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3615
Average precision score: 0.9423
              precision    recall  f1-score   support

           0       0.96      0.98      0.97       373
           1       0.97      0.99      0.98       183
           2       0.89      0.97      0.93       359
           3       0.78      0.60      0.68       155
           4       0.93      0.82      0.87       159
           5       0.85      0.89      0.87       303

    accuracy                           0.91      1532
   macro avg       0.90      0.87      0.88      1532
weighted avg       0.90      0.91      0.90      1532


Loading dataset Sensorless with preserved categories

Loading dataset Sensorless
Dataset info : 
task : classification
sample count (train + test) : 58509 with 40956 for training
number of features : 48 
 of which continuous : 48
One hot encoded features : False
number of classes : 11
class proportions : 
[0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909
 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909]


Benchmarking catboost_classifier on dataset Sensorless

fitted in 4.405s
predicted in 0.027s
ROC AUC: 0.9999, ACC: 0.9952
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.0230
Average precision score: 0.9995
              precision    recall  f1-score   support

           0       1.00      0.99      0.99      1596
           1       1.00      0.99      0.99      1596
           2       1.00      1.00      1.00      1595
           3       1.00      1.00      1.00      1596
           4       0.99      0.99      0.99      1595
           5       0.99      0.99      0.99      1596
           6       1.00      1.00      1.00      1596
           7       1.00      1.00      1.00      1596
           8       0.99      0.99      0.99      1596
           9       0.99      1.00      0.99      1595
          10       1.00      1.00      1.00      1596

    accuracy                           1.00     17553
   macro avg       1.00      1.00      1.00     17553
weighted avg       1.00      1.00      1.00     17553


Benchmarking lightgbm_classifier on dataset Sensorless

fitted in 3.130s
predicted in 0.215s
ROC AUC: 1.0000, ACC: 0.9990
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.0045
Average precision score: 1.0000
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      1596
           1       1.00      1.00      1.00      1596
           2       1.00      1.00      1.00      1595
           3       1.00      1.00      1.00      1596
           4       1.00      1.00      1.00      1595
           5       1.00      1.00      1.00      1596
           6       1.00      1.00      1.00      1596
           7       1.00      1.00      1.00      1596
           8       1.00      1.00      1.00      1596
           9       1.00      1.00      1.00      1595
          10       1.00      1.00      1.00      1596

    accuracy                           1.00     17553
   macro avg       1.00      1.00      1.00     17553
weighted avg       1.00      1.00      1.00     17553


Loading dataset Sensorless with one-hot categories

Loading dataset Sensorless
Dataset info : 
task : classification
sample count (train + test) : 58509 with 40956 for training
number of features : 48 
 of which continuous : 48
One hot encoded features : True
number of classes : 11
class proportions : 
[0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909
 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909]


Benchmarking logistic_regression_classifier on dataset Sensorless

fitted in 1.948s
predicted in 0.008s
ROC AUC: 0.9840, ACC: 0.8281
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.5920
Average precision score: 0.8551
              precision    recall  f1-score   support

           0       0.81      0.89      0.85      1596
           1       0.83      0.83      0.83      1596
           2       0.87      0.94      0.90      1595
           3       0.85      0.81      0.83      1596
           4       0.73      0.70      0.71      1595
           5       0.65      0.62      0.64      1596
           6       0.98      0.97      0.98      1596
           7       0.82      0.84      0.83      1596
           8       0.72      0.67      0.69      1596
           9       0.84      0.84      0.84      1595
          10       1.00      1.00      1.00      1596

    accuracy                           0.83     17553
   macro avg       0.83      0.83      0.83     17553
weighted avg       0.83      0.83      0.83     17553


Benchmarking xgboost_classifier on dataset Sensorless

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:04:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 32.780s
predicted in 0.056s
ROC AUC: 1.0000, ACC: 0.9980
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.0081
Average precision score: 0.9999
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      1596
           1       1.00      0.99      1.00      1596
           2       1.00      1.00      1.00      1595
           3       1.00      1.00      1.00      1596
           4       1.00      1.00      1.00      1595
           5       1.00      1.00      1.00      1596
           6       1.00      1.00      1.00      1596
           7       1.00      1.00      1.00      1596
           8       1.00      1.00      1.00      1596
           9       0.99      1.00      1.00      1595
          10       1.00      1.00      1.00      1596

    accuracy                           1.00     17553
   macro avg       1.00      1.00      1.00     17553
weighted avg       1.00      1.00      1.00     17553


Benchmarking sklearn_random_forest_classifier on dataset Sensorless

fitted in 2.921s
predicted in 0.047s
ROC AUC: 1.0000, ACC: 0.9984
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.0293
Average precision score: 1.0000
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      1596
           1       1.00      0.99      1.00      1596
           2       1.00      1.00      1.00      1595
           3       1.00      1.00      1.00      1596
           4       1.00      1.00      1.00      1595
           5       1.00      1.00      1.00      1596
           6       1.00      1.00      1.00      1596
           7       1.00      1.00      1.00      1596
           8       1.00      1.00      1.00      1596
           9       0.99      1.00      1.00      1595
          10       1.00      1.00      1.00      1596

    accuracy                           1.00     17553
   macro avg       1.00      1.00      1.00     17553
weighted avg       1.00      1.00      1.00     17553


Benchmarking wildwood_classifier on dataset Sensorless

fitted in 1.539s
predicted in 2.942s
ROC AUC: 1.0000, ACC: 0.9969
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1177
Average precision score: 0.9998
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      1596
           1       1.00      0.99      1.00      1596
           2       0.99      1.00      1.00      1595
           3       1.00      1.00      1.00      1596
           4       1.00      0.99      0.99      1595
           5       1.00      1.00      1.00      1596
           6       1.00      1.00      1.00      1596
           7       0.99      1.00      1.00      1596
           8       1.00      0.99      1.00      1596
           9       0.99      1.00      1.00      1595
          10       1.00      1.00      1.00      1596

    accuracy                           1.00     17553
   macro avg       1.00      1.00      1.00     17553
weighted avg       1.00      1.00      1.00     17553


Loading dataset Spambase with preserved categories

Loading dataset Spambase
Dataset info : 
task : classification
sample count (train + test) : 4601 with 3220 for training
number of features : 57 
 of which continuous : 57
One hot encoded features : False
number of classes : 2
class proportions : 
[0.60595523 0.39404477]


Benchmarking catboost_classifier on dataset Spambase

fitted in 0.385s
predicted in 0.003s
ROC AUC: 0.9895, ACC: 0.9544
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1298
Average precision score: 0.9789
              precision    recall  f1-score   support

           0       0.98      0.95      0.96       837
           1       0.92      0.96      0.94       544

    accuracy                           0.95      1381
   macro avg       0.95      0.96      0.95      1381
weighted avg       0.96      0.95      0.95      1381


Benchmarking lightgbm_classifier on dataset Spambase

fitted in 0.151s
predicted in 0.005s
ROC AUC: 0.9901, ACC: 0.9551
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1210
Average precision score: 0.9814
              precision    recall  f1-score   support

           0       0.97      0.95      0.96       837
           1       0.93      0.96      0.94       544

    accuracy                           0.96      1381
   macro avg       0.95      0.96      0.95      1381
weighted avg       0.96      0.96      0.96      1381


Loading dataset Spambase with one-hot categories

Loading dataset Spambase
Dataset info : 
task : classification
sample count (train + test) : 4601 with 3220 for training
number of features : 57 
 of which continuous : 57
One hot encoded features : True
number of classes : 2
class proportions : 
[0.60595523 0.39404477]


Benchmarking logistic_regression_classifier on dataset Spambase

fitted in 0.159s
predicted in 0.001s
ROC AUC: 0.9550, ACC: 0.9124
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.3290
Average precision score: 0.9266
              precision    recall  f1-score   support

           0       0.93      0.93      0.93       837
           1       0.89      0.89      0.89       544

    accuracy                           0.91      1381
   macro avg       0.91      0.91      0.91      1381
weighted avg       0.91      0.91      0.91      1381


Benchmarking xgboost_classifier on dataset Spambase

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:04:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 0.306s
predicted in 0.002s
ROC AUC: 0.9883, ACC: 0.9573
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1305
Average precision score: 0.9779
              precision    recall  f1-score   support

           0       0.97      0.96      0.96       837
           1       0.94      0.95      0.95       544

    accuracy                           0.96      1381
   macro avg       0.95      0.96      0.96      1381
weighted avg       0.96      0.96      0.96      1381


Benchmarking sklearn_random_forest_classifier on dataset Spambase

fitted in 0.158s
predicted in 0.020s
ROC AUC: 0.9882, ACC: 0.9566
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.2352
Average precision score: 0.9832
              precision    recall  f1-score   support

           0       0.96      0.97      0.96       837
           1       0.95      0.94      0.94       544

    accuracy                           0.96      1381
   macro avg       0.96      0.95      0.95      1381
weighted avg       0.96      0.96      0.96      1381


Benchmarking wildwood_classifier on dataset Spambase

fitted in 0.220s
predicted in 0.235s
ROC AUC: 0.9849, ACC: 0.9421
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1909
Average precision score: 0.9806
              precision    recall  f1-score   support

           0       0.94      0.97      0.95       837
           1       0.95      0.90      0.92       544

    accuracy                           0.94      1381
   macro avg       0.94      0.94      0.94      1381
weighted avg       0.94      0.94      0.94      1381


Loading dataset BreastCancer with preserved categories

Loading dataset BreastCancer
Dataset info : 
task : classification
sample count (train + test) : 569 with 398 for training
number of features : 30 
 of which continuous : 30
One hot encoded features : False
number of classes : 2
class proportions : 
[0.37258348 0.62741652]


Benchmarking catboost_classifier on dataset BreastCancer

fitted in 0.241s
predicted in 0.002s
ROC AUC: 0.9877, ACC: 0.9474
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1352
Average precision score: 0.9919
              precision    recall  f1-score   support

           0       0.92      0.94      0.93        64
           1       0.96      0.95      0.96       107

    accuracy                           0.95       171
   macro avg       0.94      0.95      0.94       171
weighted avg       0.95      0.95      0.95       171


Benchmarking lightgbm_classifier on dataset BreastCancer

fitted in 0.070s
predicted in 0.002s
ROC AUC: 0.9876, ACC: 0.9415
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1956
Average precision score: 0.9919
              precision    recall  f1-score   support

           0       0.91      0.94      0.92        64
           1       0.96      0.94      0.95       107

    accuracy                           0.94       171
   macro avg       0.94      0.94      0.94       171
weighted avg       0.94      0.94      0.94       171


Loading dataset BreastCancer with one-hot categories

Loading dataset BreastCancer
Dataset info : 
task : classification
sample count (train + test) : 569 with 398 for training
number of features : 30 
 of which continuous : 30
One hot encoded features : False
number of classes : 2
class proportions : 
[0.37258348 0.62741652]


Benchmarking logistic_regression_classifier on dataset BreastCancer

fitted in 0.006s
predicted in 0.001s
ROC AUC: 0.9860, ACC: 0.9474
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1738
Average precision score: 0.9911
              precision    recall  f1-score   support

           0       0.92      0.94      0.93        64
           1       0.96      0.95      0.96       107

    accuracy                           0.95       171
   macro avg       0.94      0.95      0.94       171
weighted avg       0.95      0.95      0.95       171


Benchmarking xgboost_classifier on dataset BreastCancer

Training XGBoost classifier ...
Running XGBClassifier with use_label_encoder=False
[17:04:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
fitted in 0.083s
predicted in 0.001s
ROC AUC: 0.9879, ACC: 0.9532
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1323
Average precision score: 0.9916
              precision    recall  f1-score   support

           0       0.95      0.92      0.94        64
           1       0.95      0.97      0.96       107

    accuracy                           0.95       171
   macro avg       0.95      0.95      0.95       171
weighted avg       0.95      0.95      0.95       171


Benchmarking sklearn_random_forest_classifier on dataset BreastCancer

fitted in 0.112s
predicted in 0.011s
ROC AUC: 0.9839, ACC: 0.9532
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1442
Average precision score: 0.9883
              precision    recall  f1-score   support

           0       0.97      0.91      0.94        64
           1       0.95      0.98      0.96       107

    accuracy                           0.95       171
   macro avg       0.96      0.94      0.95       171
weighted avg       0.95      0.95      0.95       171


Benchmarking wildwood_classifier on dataset BreastCancer

fitted in 0.043s
predicted in 0.010s
ROC AUC: 0.9810, ACC: 0.9357
ROC AUC computed with multi_class='ovo' (see sklearn docs)
Log loss: 0.1685
Average precision score: 0.9866
              precision    recall  f1-score   support

           0       0.93      0.89      0.91        64
           1       0.94      0.96      0.95       107

    accuracy                           0.94       171
   macro avg       0.94      0.93      0.93       171
weighted avg       0.94      0.94      0.94       171

